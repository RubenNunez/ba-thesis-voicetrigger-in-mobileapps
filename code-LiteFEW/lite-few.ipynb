{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on Publication:\n",
    "\n",
    "\"LIGHTWEIGHTFEATUREENCODERFORWAKE-UPWORDDETECTION BASEDONSELF-SUPERVISEDSPEECHREPRESENTATION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiteFEW Architecture\n",
    "class LiteFEW(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(LiteFEW, self).__init__()\n",
    "        channels = [int(512 * alpha)] * 7\n",
    "        strides = [5, 2, 2, 2, 2, 2, 2]\n",
    "        kernel_widths = [10, 3, 3, 3, 3, 2, 2]\n",
    "        \n",
    "        # Adjusting the number of input channels for each layer\n",
    "        in_channels = 1  # Initial number of channels is 1 (mono audio)\n",
    "        for c, s, k in zip(channels, strides, kernel_widths):\n",
    "            self.layers.append(nn.Conv1d(in_channels, c, k, stride=s))\n",
    "            in_channels = c  # Update in_channels for the next iteration\n",
    "            self.layers.append(nn.ReLU())\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation Training\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, bottleneck_dim)\n",
    "        self.decoder = nn.Linear(bottleneck_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "def distillation_loss(z_s, z_t, autoencoder, lambda_value=0.5):\n",
    "    z_r = autoencoder(z_t)\n",
    "    l_recon = nn.MSELoss()(z_t, z_r)\n",
    "    l_distill = nn.MSELoss()(z_s, z_r)\n",
    "    return lambda_value * l_recon + (1 - lambda_value) * l_distill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "def focal_loss(input, target, gamma=2):\n",
    "    pt = torch.sigmoid(input)\n",
    "    pt = pt if target == 1 else 1 - pt\n",
    "    return - (1 - pt) ** gamma * torch.log(pt)\n",
    "\n",
    "class WWD(nn.Module):\n",
    "    def __init__(self, litefew, num_classes=2):\n",
    "        super(WWD, self).__init__()\n",
    "        self.litefew = litefew\n",
    "        self.fc = nn.Linear(litefew.layers[-2].out_channels, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.litefew(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SAMPLE_RATE = 16000 \n",
    "TARGET_LENGTH = 2 * SAMPLE_RATE  # 2 seconds at 16kHz\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Collect all audio paths and their corresponding labels\n",
    "        self.audio_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Assuming two folders \"other\" and \"Hey_FOOBY\" representing classes 0 and 1 respectively\n",
    "        for class_label, class_name in enumerate([\"other\", \"Hey_FOOBY\"]):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith(\".wav\"):\n",
    "                    self.audio_paths.append(os.path.join(class_dir, filename))\n",
    "                    self.labels.append(class_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        waveform, _ = torchaudio.load(self.audio_paths[index])\n",
    "        \n",
    "        if waveform.size(1) < TARGET_LENGTH:\n",
    "            # Zero padding for shorter clips\n",
    "            padding_size = TARGET_LENGTH - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding_size)).squeeze(0)\n",
    "        elif waveform.size(1) > TARGET_LENGTH:\n",
    "            # Trimming for longer clips\n",
    "            waveform = waveform[:, :TARGET_LENGTH].squeeze(0)\n",
    "\n",
    "        # if self.transform:\n",
    "        #     waveform = self.transform(waveform)\n",
    "        \n",
    "        return waveform, self.labels[index]\n",
    "        \n",
    "    # def collate_fn(batch):\n",
    "    #     data, targets = zip(*batch)\n",
    "    #     min_length = min([waveform.size(2) for waveform in data])\n",
    "    #     data = [waveform[..., :min_length] for waveform in data]\n",
    "    #     return torch.stack(data), torch.tensor(targets)\n",
    "\n",
    "# Placeholder transform\n",
    "# transform = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "# Initialize dataset and loader using the root directory containing the \"other\" and \"Hey_FOOBY\" folders\n",
    "root_dir = \"/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/data-wakeup-LSTM\" \n",
    "dataset = AudioDataset(root_dir, transform=None)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True) # collate_fn=AudioDataset.collate_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Teaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2b7ca9e50>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(wav2vec2)\n",
    "train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LiteFEW' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bundle \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mpipelines\u001b[39m.\u001b[39mWAV2VEC2_ASR_BASE_960H\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m wav2vec2 \u001b[39m=\u001b[39m bundle\u001b[39m.\u001b[39mget_model() \u001b[39m# teacher model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m litefew \u001b[39m=\u001b[39m LiteFEW(alpha\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m autoencoder \u001b[39m=\u001b[39m AutoEncoder(input_dim\u001b[39m=\u001b[39mwav2vec2\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mfeature_projection\u001b[39m.\u001b[39mprojection\u001b[39m.\u001b[39min_features, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                           bottleneck_dim\u001b[39m=\u001b[39mlitefew\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mout_channels)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(\u001b[39mlist\u001b[39m(litefew\u001b[39m.\u001b[39mparameters()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(autoencoder\u001b[39m.\u001b[39mparameters()), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n",
      "\u001b[1;32m/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m in_channels \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# Initial number of channels is 1 (mono audio)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m c, s, k \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(channels, strides, kernel_widths):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39mappend(nn\u001b[39m.\u001b[39mConv1d(in_channels, c, k, stride\u001b[39m=\u001b[39ms))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     in_channels \u001b[39m=\u001b[39m c  \u001b[39m# Update in_channels for the next iteration\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mappend(nn\u001b[39m.\u001b[39mReLU())\n",
      "File \u001b[0;32m~/anaconda3/envs/ba-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LiteFEW' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "wav2vec2 = bundle.get_model() # teacher model\n",
    "\n",
    "litefew = LiteFEW(alpha=0.5)\n",
    "autoencoder = AutoEncoder(input_dim=wav2vec2.encoder.feature_projection.projection.in_features, \n",
    "                          bottleneck_dim=litefew.layers[-2].out_channels)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(list(litefew.parameters()) + list(autoencoder.parameters()), lr=0.001)\n",
    "\n",
    "# If you have a CUDA device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "litefew = litefew.to(device)\n",
    "wav2vec2 = wav2vec2.to(device)\n",
    "autoencoder = autoencoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 1, 10], expected input[1, 32, 32000] to have 1 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     train_distillation(epoch, wav2vec2, litefew, autoencoder, optimizer, train_loader)\n",
      "\u001b[1;32m/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     z_t \u001b[39m=\u001b[39m teacher\u001b[39m.\u001b[39mfeature_extractor(data, lengths)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()  \u001b[39m# Extract features using teacher\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m z_s \u001b[39m=\u001b[39m student(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m distillation_loss(z_s, z_t, autoencoder, lambda_value)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/ba-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ba-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruben/Projects/ba-thesis-voicetrigger-in-mobileapps/code-LiteFEW/lite-few.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/ba-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ba-environment/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ba-environment/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/ba-environment/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 1, 10], expected input[1, 32, 32000] to have 1 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "def train_distillation(epoch, teacher, student, autoencoder, optimizer, train_loader, lambda_value=0.5):\n",
    "    teacher.eval()  # Set teacher to evaluation mode\n",
    "    student.train() # Set student to training mode\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute lengths (assuming your data has shape [batch, time])\n",
    "        lengths = torch.full((data.size(0),), data.size(1), dtype=torch.long, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z_t = teacher.feature_extractor(data, lengths)[0].detach()  # Extract features using teacher\n",
    "        \n",
    "        z_s = student(data)\n",
    "        \n",
    "        loss = distillation_loss(z_s, z_t, autoencoder, lambda_value)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}] \"\n",
    "                  f\"Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_distillation(epoch, wav2vec2, litefew, autoencoder, optimizer, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
